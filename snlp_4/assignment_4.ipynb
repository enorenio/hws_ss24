{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e804bc2f",
   "metadata": {},
   "source": [
    "# SNLP Assignment 4\n",
    "\n",
    "Name 1: <br>\n",
    "Student id 1: <br>\n",
    "Email 1: <br>\n",
    "\n",
    "Name 2: <br>\n",
    "Student id 2:  <br>\n",
    "Email 2:  <br>\n",
    "\n",
    "Name 3: <br>\n",
    "Student id 3:  <br>\n",
    "Email 3: <br>\n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. <br>\n",
    "\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77126da3",
   "metadata": {},
   "source": [
    "# 1. Code (Theory) (1 Point)\n",
    "\n",
    "**1.1 (0.25 points)** Look at the lecture slides of chapter 4 (p. 26 - 36) and nicely write down the formulae in the following Markdown cell for these three:\n",
    "- Kraft's inequality \n",
    "- expected length per word after encoding\n",
    "- optimal length of code words\n",
    "\n",
    "**1.2 (0.25 points)** What is a prefix code and how does it relate to Kraft’s inequality?\n",
    "\n",
    "**1.3 (0.5 points)** Show mathematically that every prefix code with fixed-length satisfies Kraft's inequality. Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe7f5e",
   "metadata": {},
   "source": [
    "### Answers 1.1 - 1.3:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3911c4e0",
   "metadata": {},
   "source": [
    "# 2. Code (practical exercise) (3 Points)\n",
    "\n",
    "**2.1 (2 points)** Read up on *`Shannon–Fano coding`* on Wikipedia (https://en.wikipedia.org/wiki/Shannon-fano) and implement the algorithm that is described in the section *`Fano's code: binary splitting`*. Feel free to use the example from the lecture to check your implementation for correctness:\n",
    "\n",
    "| word | frequency | $$C(\\text{word})$$ |\n",
    "| ---- | --------- | ------- |\n",
    "|\"the\" | 0.5       | `0`     |\n",
    "|\"and\" | 0.25      | `10`    |\n",
    "|\"of\"  | 0.125     | `110`   |\n",
    "|\"he\"  | 0.125     | `111`   |\n",
    "\n",
    "**2.2 (0.5 points)** Load the text file `alice_eng.txt` and preprocess it by lowercasing, removing punctuation and tokenizing the words by whitespaces. Now take the first 500 words of that corpus and run your implementation of Shannon-Fano encoding on it. Compute and print out the average amount of bits per word.\n",
    "\n",
    "**2.3 (0.5 points)** Explain the differences between Shannon-Fano coding and Huffman coding. Provide an example where Huffman coding is more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef4498",
   "metadata": {},
   "source": [
    "### 2.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34baa2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO:\n",
    "\n",
    "def get_encoding(symbols: dict, base: int = 2) -> dict:\n",
    "    \"\"\" Generate a prefix encoding for each of the symbols\n",
    "\n",
    "    Args:\n",
    "    alphabet - a dictionary that holds in its keys the alphabet item and\n",
    "                its relative frequency as the value, e.g. {'a': 0.5, 'b': 0.5}\n",
    "    base - an `int` that represents the base of the code. Feel free to make\n",
    "            your implementation open to other bases. For the purpose of this\n",
    "            exercise, using only base `2`, i.e. binary code, suffices.\n",
    "\n",
    "    Returns a `dict` with alphabet items in its keys and the encoding as the\n",
    "    values, e.g. {'a': '0', 'b': '1'}\n",
    "    \"\"\"\n",
    "        \n",
    "    raise NotImplementedError\n",
    "    \n",
    "print(get_encoding({\"the\": 0.5, \"and\": 0.25, \"of\": 0.125, \"he\": 0.125}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813d001",
   "metadata": {},
   "source": [
    "### 2.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5c7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff7ee5",
   "metadata": {},
   "source": [
    "### Answer 2.3:\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c54e79",
   "metadata": {},
   "source": [
    "# 3. Perplexity Calculations (3 Points)\n",
    "\n",
    "You are given a vocabulary of three words $w_1, w_2, w_3$ and a language model that assigns probabilities to these words:<br>\n",
    "| word | $$p(w_i)$$ |\n",
    "| ---- | --------- |\n",
    "|$$w_1$$ | $$0.2$$       |\n",
    "|$$w2$$ | $$0.5$$      |\n",
    "|$$w3$$  | $$0.3$$     |\n",
    "\n",
    "**3.1 (0.25 points)** Calculate the entropy.\n",
    "\n",
    "**3.2 (0.25 points)** Calculate the perplexity.\n",
    "\n",
    "**3.3 (1 point)** Write down the Lagrangian function that minimizes the entropy under the constraint that the probabilities sum up to 1.\n",
    "\n",
    "$$L(p_1, p_2, p_3, λ) = $$\n",
    "\n",
    "**3.4 (1 point)** Write down the four partial derivatives and set them to zero. Calculate the result for the optimized probabilities.\n",
    "\n",
    "**3.5 (0.5 points)** Now compute the perplexity for the optimized probabilities. Explain the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ca683e",
   "metadata": {},
   "source": [
    "### Answers 3.1 - 3.5:\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27201869",
   "metadata": {},
   "source": [
    "# 4. OOVs (3 points)\n",
    "\n",
    "**4.1 (0.25 points)** What are out-of-vocabulary words (OOVs)?\n",
    "\n",
    "**4.2 (0.25 points)** How does perplexity behave with words that are less frequent?\n",
    "\n",
    "**4.3 (0.25 points)** Show mathematically what would happen to the perplexity if a symbol in the test set is an OOV. In this case, assume a probability of $0^+$.\n",
    "\n",
    "**4.4 (0.25 points)** Load the Penn Treebank in NLTK (based on the WSJ corpus), preprocess it by lowercasing and removing punctuation, and then perform a 70:30 train:test split.\n",
    "\n",
    "The go-to solution for modelling OOVs in the N-gram setting is to introduce a new `<unk>` token in the vocabulary for all unknown words. The `<unk>` token replaces all OOVs and is then modeled like a normal word. \n",
    "\n",
    "**4.5 (0.5 points)** Your task is to complete the function to create a vocabulary with the top_n most frequent words in the train set.\n",
    "\n",
    "**4.6 (0.5 points)** Complete the function that restricts a corpus into the given vocabulary, i.e. all OOVs will be replaced with `<unk>`.\n",
    "\n",
    "**4.7 (0.5 points)** Plot the OOV-rates (see chapter 5 p. 5) for the train and test set with increasing size of the vocabulary (use a log-log scale). What do you observe?\n",
    "\n",
    "**4.8 (0.5 points)** Plot the perplexity for the train and test set with increasing size of the vocabulary. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f58f06",
   "metadata": {},
   "source": [
    "### Answers 4.1 - 4.3:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e3385",
   "metadata": {},
   "source": [
    "### Your Code for 4.4 - 4.6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5403a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "\n",
    "### TODO\n",
    "\n",
    "def load_and_preprocess_data() -> list[list[str]]:\n",
    "    ''' Function that loads the WSJ dataset, removes punctuation and lowercases the tokens\n",
    "    Output: the preprocessed corpus - list[list[str]]'''\n",
    "    raise NotImplementedError\n",
    "\n",
    "def train_test_split(corpus: list[list[str]]) -> (list[list[str]], list[list[str]]):\n",
    "    '''Splits the corpus using a 70:30 ratio. Do not randomize anything here. use the original order\n",
    "    Input: corpus - list[list[str]]\n",
    "    Output: tuple of train and test set - list[list[str]], list[list[str]]'''\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def make_vocab(corpus: list[list[str]], top_n: int) -> set:\n",
    "    '''Make the top_n frequent vocabulary from a corpus\n",
    "    Input: corpus - list[list[str]]\n",
    "         top_n  - int\n",
    "    Output: the vocabulary - set of words'''\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def restrict_vocab(corpus: list[list[str]], vocab: set) -> list[list[str]]:\n",
    "    '''Make the corpus fit inside the vocabulary using <unk>\n",
    "    Input: corpus - list[list[str]]\n",
    "         vocab  - set of words\n",
    "    Output: The corpus restricted by the vocabulary - list[list[str]]'''\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00799b6a",
   "metadata": {},
   "source": [
    "### Your Code for 4.7 - 4.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "345d5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for top_n in np.linspace(100,5000,10,dtype=int):\n",
    "    ### TODO: plot\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ee4acd",
   "metadata": {},
   "source": [
    "### Answers 4.7 - 4.8:\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc8d29",
   "metadata": {},
   "source": [
    "# 5. Bonus: Cracking Caesar Cipher (3 Points)\n",
    "\n",
    "**5.1 (0.5 point)** Load the files `alice_eng.txt`, `alice_fra.txt` and `alice_ger.txt`. Preprocess them by lowercasing and removing punctuation. Now replace every special language-specific character that is not a letter of the Latin alphabet with the special token '?', but keep the whitespaces.\n",
    "\n",
    "**5.2 (0.5 point)** Perform a frequency analysis of the remaining characters and sort them in descending order. Plot a histogram for each language.\n",
    "\n",
    "**5.3 (2 point)** You are given a text that was encrypted using the Caesar cipher (https://en.wikipedia.org/wiki/Caesar_cipher) with an unknown key. The original language is either English, French or German. Analyze the letter frequency of this text and then compute the Kullback-Leibler divergence against the three languages to find out which language it is.\n",
    "\n",
    "\"ul tyrdgzxefe uzk cr tyvezccv tfddv jz rcztv rmrzk gric? kflk yrlkvk le dfdvek rgi?j cr tyvezccv rmrzk uzjgrilrcztv tfekvdgcr cv tyrdgzxefe ule rzi gvejzw gveurek le zejkrek vjjrprek uv uvmzevi hlvcj ve ?krzvek cvj t?k?j vk tfddv cv tyrdgzxefe?krzk kflk ifeu vccv kiflmr cr hlvjkzfe wfik vdsriirjjrekv vewze vccv?kveuzk jvj sirj kflk rlkfli ve cvj rccfexvrek rlkrek hlv gfjjzscvvk uv tyrhlv drze vecvmr lev gvkzkv grikzv ul sfiu ul tyrdgzxefe drzekverek cvhlvc uvj uvlo jv uzkvccv vk vccv xizxefkr le gvl ul dfitvrl uv cr drze uifzkv gfli mfzi hlvc vwwvk zc gifulzirzk givjhlv rljjzk?k vccv iv?lk le tflg mzfcvek jflj cv dvekfe zc mverzk uv wirggvi tfekiv jfe gzvu tv siljhlv tyrexvdvek clz wzk xireu gvli drzj vccv tfdgizk hlzc ep rmrzk grj uv kvdgj ? gviuiv tri vccv uzdzelrzk irgzuvdvek vccv jv dzk ufet szve mzkv ? drexvi le gvl uv crlkiv dfitvrl jfe dvekfe ?krzk jz irggifty? uv jfe gzvu hlzc p rmrzk ? gvzev rjjvq uv gcrtv gfli hlvccv g?k flmizi cr sfltyv vccv p i?ljjzk vewze vk grimzek ? rmrcvi lev grikzv ul dfitvrl uv cr drze xrltyv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc86655",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
