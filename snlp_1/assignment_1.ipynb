{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-21tlRxnfLW"
   },
   "source": [
    "# SNLP Assignment 1\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o_hqMzc7qAH"
   },
   "source": [
    "## Exercise 1 (7 = 2+2+0.25+0.25+0.5+1+1 points)\n",
    "\n",
    "You are given the subfolder `data`, which contains 4 files:\n",
    "- `alice_eng.txt` contains the text of Alice's Adventures in Wonderland by Lewis Carroll (source: https://www.gutenberg.org/ebooks/19033).\n",
    "- `alice_ger.txt` contains the translation of Alice's Adventures in Wonderland into German (source: https://www.gutenberg.org/ebooks/19778).\n",
    "- `trainer.py` contains the Python code of Transformer's trainer module by Huggingface (source: https://github.com/huggingface/transformers/blob/v4.40.1/src/transformers/trainer.py#L277).\n",
    "- `uniprot_sprot.fasta` contains sequences of amino acids, forming proteins (source: https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz). If you are interested, you can look up the meaning of the letters inside the file here: https://en.wikipedia.org/wiki/Protein_sequence\n",
    "\n",
    "---\n",
    "\n",
    "We provide you with some code that loads the data and tokenizes the text (such that you receive a list of words). Your task is to implement the functions `analysis_linear` and `analysis_loglog`, as well as answer the questions.\n",
    "\n",
    "\n",
    "### 1. Linear Plot for `alice_eng.txt` (2 points)\n",
    "- (1.1) Count how many times each word occurs (raw frequencies) in `alice_eng.txt`, then rank the words, such that highest frequency has rank 1 and so on. For each of the words, plot the frequency against the rank. Use a plot with linear axes. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here!\n",
    "# TODO:\n",
    "\n",
    "def analysis_linear(name, data):\n",
    "    \"\"\"\n",
    "    Plot frequency against rank linearly\n",
    "    \n",
    "    :param name: title of the graph\n",
    "    :param data: list of words\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on English text, linear axes\n",
    "with open(\"data/alice_eng.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    analysis_linear(\"English\", f.read().lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Log-log Plots (3 points)\n",
    "- (2.1) Now do this for all of the 4 files, but use the log-log scale for your plots. (2 = (5 * 0.5) points)\n",
    "- (2.2) Why is it better to use a log-log scale? (0.25 points)\n",
    "- (2.3) In your plot, what causes the vertical gaps (\"steps\") for high-rank words (rightmost)? (0.25 points)\n",
    "- (2.4) What are the differences between the languages? What causes them? (0.5 points)\n",
    "\n",
    "### 3. Zipf's Law (2 points)\n",
    "- (3.1) Zipf's law \"predicts\" the frequency of the n-th rank word. Add an 'ideal' line to your 4 plots, according to Zipf's law. Use the following formula. (1 point)\n",
    "\n",
    "$$\\hat{y}_i = \\frac{\\text{frequency of }word_{rank1}} {\\text{rank of } word_i}$$\n",
    "\n",
    "- (3.2) Compute the mean squared error (MSE) of these predictions, and output the value to 10 decimal digits. (1 point)\n",
    "\n",
    "$$ MSE = \\big(\\frac{1}{n} \\sum (\\hat{y}_i - y_i)^2\\big)$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answers to questions (2.2-4) go here!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for 2. and 3. goes here!\n",
    "# TODO:\n",
    "\n",
    "def analysis_loglog(name, data):\n",
    "    \"\"\"\n",
    "    Plot Zipfian distribution of words + ideal Zipfian distribution on a loglog scale.\n",
    "    Compute and print out MSE.\n",
    "\n",
    "    :param name: title of the graph\n",
    "    :param data: list of words\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize\n",
    "\n",
    "# run on English text, log-log scale\n",
    "with open(\"data/alice_eng.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    analysis_loglog(\"English\", f.read().lower().split())\n",
    "\n",
    "# run on German text, log-log scale\n",
    "with open(\"data/alice_ger.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    analysis_loglog(\"German\", f.read().lower().split())\n",
    "\n",
    "# run on Transformer's trainer module's source code, log-log scale\n",
    "with open(\"data/trainer.py\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tokens = [\n",
    "        x.string\n",
    "        for x in tokenize.generate_tokens(f.readline)\n",
    "        if x.type not in {\n",
    "            tokenize.COMMENT, tokenize.STRING, tokenize.INDENT, tokenize.DEDENT, tokenize.NEWLINE\n",
    "        }\n",
    "    ]\n",
    "    analysis_loglog(\"Python\", tokens)\n",
    "    \n",
    "# run on Uniprot's protein data, log-log scale\n",
    "with open(\"data/uniprot_sprot.fasta\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    proteins = []\n",
    "    for line in f:\n",
    "        if line.startswith(\">\"):\n",
    "            continue\n",
    "        proteins.append(line.strip())\n",
    "    proteins_long = ''.join(proteins)\n",
    "    analysis_loglog(\"Proteins\", [*proteins_long])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (3 = 1+0.5+0.5+1 points)\n",
    "\n",
    "For these exercises, please use your own loaders similar to the ones we provided.\n",
    "\n",
    "##### Bonus 1:\n",
    "- Add another 'ideal' curve to your 4 plots, but use the Mandelbrot formula instead (lecture slides chapter_2 page 19). You should play around with the parameters a little bit, but it's not necessary to achieve a perfect fit for every plot. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for Bonus 1 goes here!\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyWwTwR0-bcq"
   },
   "source": [
    "##### Bonus 2: run these two experiments for both of the `alice` texts (independent of each other) like you did in the main exercises:\n",
    "- (Bonus 2.1) Don't lowercase anything. (0.5 points)\n",
    "- (Bonus 2.2) Use character level tokenization, rather than word level. (0.5 points)\n",
    "\n",
    "You don't have to include the Mandelbrot function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for Bonus 2.1 goes here!\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for Bonus 2.2 goes here!\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bonus 3:\n",
    "- Use NLTK's corpora and download the `Brown corpus` corpus (id: \"brown\") and create a plot like you did before. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for Bonus 3 goes here!\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
