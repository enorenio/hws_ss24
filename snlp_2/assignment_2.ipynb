{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-21tlRxnfLW"
   },
   "source": [
    "# SNLP Assignment 2\n",
    "\n",
    "Name 1: <br/>\n",
    "Student id 1: <br/>\n",
    "Email 1: <br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "Name 3: <br/>\n",
    "Student id 3: <br/>\n",
    "Email 3: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o_hqMzc7qAH"
   },
   "source": [
    "## Exercise 1 - Probability theory (0.5 + 0.5 = 1 point)\n",
    "\n",
    "Consider a fair 6-sided die whose sides are numbered from 1 to 6 and each die roll is independent of the other rolls. In an experiment that consists of rolling the die twice, the following events can be defined:\n",
    "\n",
    "    A: The sum of the two outcomes is at least 10\n",
    "    B: At least one of the two rolls resulted in 6\n",
    "\n",
    "a. Compute the probabilities $P(A)$, and $P(B)$.\n",
    "\n",
    "b. Is event A independent of event B?\n",
    "\n",
    "You can leave the solution as a fraction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Bayes theorem (0.5 + 1 = 1.5 points)\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability theory used to update the probability of a hypothesis based on new evidence. It states that the probability of a hypothesis (H) given some evidence (E) is proportional to the probability of the evidence given the hypothesis, multiplied by the prior probability of the hypothesis, divided by the probability of the evidence regardless of the hypothesis. Mathematically,\n",
    "\n",
    "$$ P(H|E) = \\frac{P(E|H)P(H)}{P(E)} $$\n",
    "\n",
    "Where,\n",
    "- $P(H∣E)$ is the posterior probability of hypothesis H given evidence E.\n",
    "- $P(E∣H)$ is the probability of observing evidence E given that the hypothesis H is true.\n",
    "- $P(H)$ is the prior probability of hypothesis H being true.\n",
    "- $P(E)$ is the probability of observing evidence E.\n",
    "\n",
    "\n",
    "Now, Imagine that all of the blogs online were generated by some Large Language Model (LLM). Of these blogs, $80\\%$ were generated using GPT-4, $15\\%$ of them were generated using LLAMA 3 and $5\\%$ were generated using Mistral. About $4\\%$ of the text generated by GPT-4 seem to have counterfactual statements. For LLAMA 3, this is about $6\\%$ and for Mistral, this is about $9\\%$.\n",
    "\n",
    "a. If a blog post is randomly selected from these online blogs, what is the probability that it was generated by LLAMA 3?\n",
    "\n",
    "b. If a randomly selected blog post contained a counterfactual statement, find the probability that it was generated using GPT-4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Miller's model (0.5 + 1 + 0.5 = 2 points)\n",
    "\n",
    "Miller's model introduced in the lecture simulates the process of random text generation at the character level. It is a memoryless model which means that the generation at each time step is independent of what has been generated so far. Imagine that there is a monkey typing on a computer keyboard. The keyboard has only 27 keys: a to z and a spacebar. Assume that the probability of each character is distributed according to its occurrence in the English language. We call a sequence of characters separated by white space as a word.\n",
    "\n",
    "a. What is the probability that the monkey will type the word _'ear'_ on the keyboard? Hint: See the `exercise_3.py` file.\n",
    "\n",
    "b. Complete the `compute_perplexity()` function in the `exercise_3.py` file to compute the perplexity of the Miller's model for the text: \"hello i am a monkey\". \n",
    "\n",
    "c. What would happen to the perplexity if we were to add a few more whitespace in between the words?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.b\n",
    "\n",
    "# DO NOT MODIFY THIS PART\n",
    "# Your code should be placed in exercise_3.py\n",
    "import exercise_3\n",
    "from importlib import reload\n",
    "\n",
    "reload(exercise_3)\n",
    "\n",
    "model = exercise_3.MillersModel()\n",
    "\n",
    "text = \"hello i am a monkey\"\n",
    "\n",
    "print(\"Perplexity: \", model.compute_perplexity(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Sampling in Language models (1 + 2 + 0.5 + 0.5 =4 points)\n",
    "\n",
    "Neural Networks are quite effective as generative models. Huggingface Transformers library can be used to easily get started with training and using these models. Take a look at the [Huggingface transformers quick tour](https://huggingface.co/docs/transformers/quicktour) to get acquainted with the transformers library. (Recommended: [Chapter 1](https://huggingface.co/learn/nlp-course/chapter1/1) and [Chapter 2](https://huggingface.co/learn/nlp-course/chapter2/1) of the Huggingface NLP Course)\n",
    "\n",
    "a. Read up on GPT2 [[1]](https://huggingface.co/openai-community/gpt2) [[2]](https://huggingface.co/docs/transformers/model_doc/gpt2). Implement the `forward()` function in the `GPT2Model` class in `exercise_4.py` file to take input text sequence and return the probability distribution of the predicted next token. The general steps would be: tokenize and encode the input text, feed the input to the model to get the logits (Note: You might have to take only the last logit which is for the predicted next token in the sequence.), and apply softmax function to convert it to a probability distribution.\n",
    "\n",
    "b. We can use different types of sampling strategies while selecting the next token for generating a text. Some common approaches are: Greedy sampling, random sampling, beam search, etc.\n",
    "\n",
    "   - Greedy sampling\n",
    "      In greedy sampling, the model selects the word with the highest probability based on the given context. It prioritizes the most likely next word based on the model's predictions, without considering other possibilities. This method often produces coherent and fluent text but can lead to repetitive or predictable outputs, as it doesn't explore alternative word choices.\n",
    "\n",
    "   - Random sampling\n",
    "      In random sampling, the next token is randomly sampled from the output probability distribution. This can make the generation non-deterministic and unpredictable but doesn't usually fall into repetitions.\n",
    "\n",
    "   Implement the `greedy_sample()` and `random_sample()` functions in `GPT2Model` class in `exercise_4.py` file for Greedy sampling and random sampling respectively. For each of them, generate a continuation of the sentence given below.\n",
    "\n",
    "c. Describe the output with each sampling method. Which one would you generally prefer?\n",
    "\n",
    "d. Another approach for generating from language models is using Beam search. Describe how the beam search works (2 sentences). Does it solve any problems of the previous sampling methods?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary packages\n",
    "!pip install transformers torch evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4\n",
    "\n",
    "# DO NOT MODIFY THIS PART\n",
    "# Your code should be placed in exercise_4.py\n",
    "import exercise_4\n",
    "from importlib import reload\n",
    "\n",
    "reload(exercise_4)\n",
    "\n",
    "model = exercise_4.GPT2Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy sampling\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.greedy_sample(text)\n",
    "\n",
    "print(\"Greedy sampled output: \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.random_sample(text)\n",
    "\n",
    "print(\"Random sampled output: \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Computing Perplexity (0.5 + 1 = 1.5 points)\n",
    "\n",
    "Huggingface provides the `evaluate` library that includes various metrics used in machine learning along with perplexity that we use in Language modeling. We can use that library to quickly compute the perplexity of a language model. There is also the `datasets` library from Huggingface that allows to easily access datasets for many different tasks.\n",
    "\n",
    "a. Read the [metric card for perplexity](https://huggingface.co/spaces/evaluate-metric/perplexity) and compute the perplexity for the following text using GPT2.\n",
    "\n",
    "b. Load the `alice_in_wonderland.txt` file using Hugginface `datasets` library and compute the perplexity for it using GPT2. Check the [documentations](https://huggingface.co/docs/datasets/index) for datasets library.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5.a\n",
    "from evaluate import load\n",
    "\n",
    "texts = [\"Eureka! I have found it!\"]\n",
    "\n",
    "# your code here\n",
    "\n",
    "\n",
    "print(\"Perplexity: \", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 5.b\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "\n",
    "# your code here\n",
    "\n",
    "print(\"Perplexity: \", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection Sampling (2 points)\n",
    "\n",
    "Rejection sampling is another approach we can use to generate the next token from the language model. In this approach, we randomly sample a token from the list and accept it if it's probability is greater than some random number and reject it otherwise.\n",
    "\n",
    "1. Generate a random number, r.\n",
    "2. Randomly sample a token from the output probability (say token at index i with probability p).\n",
    "3. Is r < p?\n",
    "\n",
    "    a. If yes, then accept i as the next token.\n",
    "    \n",
    "    b. If no, then go back to step 1.\n",
    "\n",
    "Complete the `rejection_sample()` function in the `GPT2Model` class in `exercise_4.py`. How does the generated output differ from the greedy and random sampling?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.random_sample(text)\n",
    "\n",
    "print(\"Rejection sampled output: \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity with different languages (0.5 + 0.5 = 1 point)\n",
    "\n",
    "a. In exercise 5.a., what would happen if we were to use a different language other than English? How would the perplexity be affected? Test this with another language of your choice.\n",
    "\n",
    "b. Are there any pre-trained model for the language you selected? Try with that model to see if you can get a better perplexity score. If there are no pre-trained LM for that language, try with a multilingual LM and see if the perplexity score improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
