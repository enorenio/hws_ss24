{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-21tlRxnfLW"
   },
   "source": [
    "# SNLP Assignment 2\n",
    "\n",
    "Name 1: Zoha Zehra<br/>\n",
    "Student id 1: 7061946<br/>\n",
    "Email 1: zoze00001@stud.uni-saarland.de<br/>\n",
    "\n",
    "\n",
    "Name 2: Daniya Niazi<br/>\n",
    "Student id 2: 7062343<br/>\n",
    "Email 2: dani00003@stud.uni-saarland.de<br/> \n",
    "\n",
    "Name 3: Aleksey Morshnev<br/>\n",
    "Student id 3:7042691<br/>\n",
    "Email 3: almo00008@stud.uni-saarland.de<br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o_hqMzc7qAH"
   },
   "source": [
    "## Exercise 1 - Probability theory (0.5 + 0.5 = 1 point)\n",
    "\n",
    "Consider a fair 6-sided die whose sides are numbered from 1 to 6 and each die roll is independent of the other rolls. In an experiment that consists of rolling the die twice, the following events can be defined:\n",
    "\n",
    "    A: The sum of the two outcomes is at least 10\n",
    "    B: At least one of the two rolls resulted in 6\n",
    "\n",
    "a. Compute the probabilities $P(A)$, and $P(B)$.\n",
    "\n",
    "b. Is event A independent of event B?\n",
    "\n",
    "You can leave the solution as a fraction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1,1) \n",
    "(1,2)\n",
    "(1,3)\n",
    "(1,4)\n",
    "(1,5)\n",
    "(1,6)\n",
    "\n",
    "(2,1)\n",
    "(2,2)\n",
    "(2,3)\n",
    "(2,4)\n",
    "(2,5)\n",
    "(2,6)\n",
    "\n",
    "(3,1)\n",
    "(3,2)\n",
    "(3,3)\n",
    "(3,4)\n",
    "(3,5)\n",
    "(3,6)\n",
    "\n",
    "(4,1)\n",
    "(4,2)\n",
    "(4,3)\n",
    "(4,4)\n",
    "(4,5)\n",
    "(4,6)\n",
    "\n",
    "(5,1)\n",
    "(5,2)\n",
    "(5,3)\n",
    "(5,4)\n",
    "(5,5)\n",
    "(5,6)\n",
    "\n",
    "(6,1)\n",
    "(6,2)\n",
    "(6,3)\n",
    "(6,4)\n",
    "(6,5)\n",
    "(6,6)\n",
    "\n",
    "Total Outcome: 36 \n",
    "\n",
    "\n",
    "A: The sum of the two outcomes is at least 10\n",
    "\n",
    "P(A) = 6/36 = 1/6\n",
    "\n",
    "B: At least one of the two rolls resulted in 6\n",
    "\n",
    "P(B) = 11/36\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The favorable outcomes for event A and B are:\n",
    "(4, 6), (5, 6), (6, 4), (6, 5), (6, 6)\n",
    "\n",
    "P(A and B) = 5/11\n",
    "\n",
    "P(A) × P(B) = (1/6) x (11/36) = 11/216\n",
    "\n",
    "Since P(A and B) is not equal to P(A) × P(B), events A and B are not independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Bayes theorem (0.5 + 1 = 1.5 points)\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability theory used to update the probability of a hypothesis based on new evidence. It states that the probability of a hypothesis (H) given some evidence (E) is proportional to the probability of the evidence given the hypothesis, multiplied by the prior probability of the hypothesis, divided by the probability of the evidence regardless of the hypothesis. Mathematically,\n",
    "\n",
    "$$ P(H|E) = \\frac{P(E|H)P(H)}{P(E)} $$\n",
    "\n",
    "Where,\n",
    "- $P(H∣E)$ is the posterior probability of hypothesis H given evidence E.\n",
    "- $P(E∣H)$ is the probability of observing evidence E given that the hypothesis H is true.\n",
    "- $P(H)$ is the prior probability of hypothesis H being true.\n",
    "- $P(E)$ is the probability of observing evidence E.\n",
    "\n",
    "\n",
    "Now, Imagine that all of the blogs online were generated by some Large Language Model (LLM). Of these blogs, $80\\%$ were generated using GPT-4, $15\\%$ of them were generated using LLAMA 3 and $5\\%$ were generated using Mistral. About $4\\%$ of the text generated by GPT-4 seem to have counterfactual statements. For LLAMA 3, this is about $6\\%$ and for Mistral, this is about $9\\%$.\n",
    "\n",
    "a. If a blog post is randomly selected from these online blogs, what is the probability that it was generated by LLAMA 3?\n",
    "\n",
    "b. If a randomly selected blog post contained a counterfactual statement, find the probability that it was generated using GPT-4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Generated by LLAMA 3) = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H: The blog post was generated using GPT-4\n",
    "\n",
    "E: The blog post contains a counterfactual statement\n",
    "\n",
    "P(H) = 0.8\n",
    "\n",
    "P(E|H) = 0.04\n",
    "\n",
    "P(E) = P(E|H) x P(H) + P(E|!H) x P(!H)\n",
    "\n",
    "P(E|H) = 0.04\n",
    "\n",
    "P(H) = 0.8\n",
    "\n",
    "P(!H) = 1 - 0.8\n",
    "\n",
    "P(!H) = 0.2\n",
    "\n",
    "P(E|!H) = (0.06 * 0.15 + 0.09 * 0.05) / 0.2\n",
    "\n",
    "P(E|!H) = 0.0675\n",
    "\n",
    "P(E) = (0.8 x 0.04) / 0.0675 x 0.2 = 0.0495\n",
    "\n",
    "P(H|E)  = (0.04 * 0.8) / 0.0495 \n",
    "\n",
    "P(H|E) =  0.6464\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Miller's model (0.5 + 1 + 0.5 = 2 points)\n",
    "\n",
    "Miller's model introduced in the lecture simulates the process of random text generation at the character level. It is a memoryless model which means that the generation at each time step is independent of what has been generated so far. Imagine that there is a monkey typing on a computer keyboard. The keyboard has only 27 keys: a to z and a spacebar. Assume that the probability of each character is distributed according to its occurrence in the English language. We call a sequence of characters separated by white space as a word.\n",
    "\n",
    "a. What is the probability that the monkey will type the word _'ear'_ on the keyboard? Hint: See the `exercise_3.py` file.\n",
    "\n",
    "b. Complete the `compute_perplexity()` function in the `exercise_3.py` file to compute the perplexity of the Miller's model for the text: \"hello i am a monkey\". \n",
    "\n",
    "c. What would happen to the perplexity if we were to add a few more whitespace in between the words?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 3\n",
    "\n",
    "As per millers model the random generation is at character level. </br>\n",
    "\n",
    "P('ear') = P('e') * P('a') * P('r') </br>\n",
    "\n",
    "P('ear') = 0.08929 * 0.06798 * 0.06065 = 0.00036814150922999996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  17.628396499801106\n"
     ]
    }
   ],
   "source": [
    "# Solution 3.b\n",
    "\n",
    "# DO NOT MODIFY THIS PART\n",
    "# Your code should be placed in exercise_3.py\n",
    "import exercise_3\n",
    "from importlib import reload\n",
    "\n",
    "reload(exercise_3)\n",
    "\n",
    "model = exercise_3.MillersModel()\n",
    "\n",
    "text = \"hello i am a monkey\"\n",
    "\n",
    "print(\"Perplexity: \", model.compute_perplexity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity with spaces:  15.634876233263183\n",
      "Perplexity with more spaces:  13.558257950994317\n"
     ]
    }
   ],
   "source": [
    "# Solution 3.c\n",
    "\n",
    "## lets add more space to text\n",
    "\n",
    "\n",
    "text = \"he llo i am a mon key\"\n",
    "\n",
    "print(\"Perplexity with spaces: \", model.compute_perplexity(text))\n",
    "\n",
    "text = \"he ll o i am a m on ke y\"\n",
    "\n",
    "print(\"Perplexity with more spaces: \", model.compute_perplexity(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I'm increasing the spaces the perplexity is getting decreasing indiciating higher perfomace at non meaningful arranged sequence of words. We have passed a meaningful sentence but 17.62 value is indicating that this model is still uncertain even in correct sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Sampling in Language models (1 + 2 + 0.5 + 0.5 =4 points)\n",
    "\n",
    "Neural Networks are quite effective as generative models. Huggingface Transformers library can be used to easily get started with training and using these models. Take a look at the [Huggingface transformers quick tour](https://huggingface.co/docs/transformers/quicktour) to get acquainted with the transformers library. (Recommended: [Chapter 1](https://huggingface.co/learn/nlp-course/chapter1/1) and [Chapter 2](https://huggingface.co/learn/nlp-course/chapter2/1) of the Huggingface NLP Course)\n",
    "\n",
    "a. Read up on GPT2 [[1]](https://huggingface.co/openai-community/gpt2) [[2]](https://huggingface.co/docs/transformers/model_doc/gpt2). Implement the `forward()` function in the `GPT2Model` class in `exercise_4.py` file to take input text sequence and return the probability distribution of the predicted next token. The general steps would be: tokenize and encode the input text, feed the input to the model to get the logits (Note: You might have to take only the last logit which is for the predicted next token in the sequence.), and apply softmax function to convert it to a probability distribution.\n",
    "\n",
    "b. We can use different types of sampling strategies while selecting the next token for generating a text. Some common approaches are: Greedy sampling, random sampling, beam search, etc.\n",
    "\n",
    "   - Greedy sampling\n",
    "      In greedy sampling, the model selects the word with the highest probability based on the given context. It prioritizes the most likely next word based on the model's predictions, without considering other possibilities. This method often produces coherent and fluent text but can lead to repetitive or predictable outputs, as it doesn't explore alternative word choices.\n",
    "\n",
    "   - Random sampling\n",
    "      In random sampling, the next token is randomly sampled from the output probability distribution. This can make the generation non-deterministic and unpredictable but doesn't usually fall into repetitions.\n",
    "\n",
    "   Implement the `greedy_sample()` and `random_sample()` functions in `GPT2Model` class in `exercise_4.py` file for Greedy sampling and random sampling respectively. For each of them, generate a continuation of the sentence given below.\n",
    "\n",
    "c. Describe the output with each sampling method. Which one would you generally prefer?\n",
    "\n",
    "d. Another approach for generating from language models is using Beam search. Describe how the beam search works (2 sentences). Does it solve any problems of the previous sampling methods?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary packages\n",
    "# !conda install transformers evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4\n",
    "\n",
    "# DO NOT MODIFY THIS PART\n",
    "# Your code should be placed in exercise_4.py\n",
    "import exercise_4\n",
    "from importlib import reload\n",
    "\n",
    "reload(exercise_4)\n",
    "\n",
    "model = exercise_4.GPT2Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.forward(\"Alice was beginning to get very tired of sitting by her sister on the bank \")\n",
    "predictions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy sampled output:  Alice was beginning to get very tired of sitting by her sister on the bank   and she was starting to get tired of sitting\n"
     ]
    }
   ],
   "source": [
    "# Greedy sampling\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.greedy_sample(text)\n",
    "\n",
    "print(\"Greedy sampled output: \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sampled output:  Alice was beginning to get very tired of sitting by her sister on the bank urchin and while she tried to talk to her\n"
     ]
    }
   ],
   "source": [
    "# Random sampling\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.random_sample(text)\n",
    "\n",
    "print(\"Random sampled output: \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.c\n",
    "\n",
    "Greedy sample use the information that Alice is sitting and now she is tired of sitting ( knowing that she is sitting ) however random sample text do not make any meaning with the alice being sitting near the bank and trying to get something done.\n",
    "\n",
    "I would prefer greedy sample for text generation scenarios and for random text generation randam sample would work fine.\n",
    "\n",
    "#### 4.d Beam search\n",
    "\n",
    "Beam search works by maintaing history of n best candidates ( n is the beam width ) for the resulting output text. This could provide the results from n best combination instead if focusing on only 1 best predictions and never explored the outcome of other candidtate further.\n",
    "\n",
    "Previous sampling method could lead to single dead end where further text sense is lost. In case of beam search we can back track to second best search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Computing Perplexity (0.5 + 1 = 1.5 points)\n",
    "\n",
    "Huggingface provides the `evaluate` library that includes various metrics used in machine learning along with perplexity that we use in Language modeling. We can use that library to quickly compute the perplexity of a language model. There is also the `datasets` library from Huggingface that allows to easily access datasets for many different tasks.\n",
    "\n",
    "a. Read the [metric card for perplexity](https://huggingface.co/spaces/evaluate-metric/perplexity) and compute the perplexity for the following text using GPT2.\n",
    "\n",
    "b. Load the `alice_in_wonderland.txt` file using Hugginface `datasets` library and compute the perplexity for it using GPT2. Check the [documentations](https://huggingface.co/docs/datasets/index) for datasets library.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334e27d56f5a437a8a940469f6631990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  24.384519577026367\n"
     ]
    }
   ],
   "source": [
    "# Solution 5.a\n",
    "from evaluate import load\n",
    "\n",
    "texts = [\"Eureka! I have found it!\"]\n",
    "\n",
    "# your code here\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(predictions=texts, model_id='gpt2')\n",
    "\n",
    "\n",
    "print(\"Perplexity: \",results['mean_perplexity'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af457258953747008981fa10e9c44794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Perplexity:  633.9214832999489\n"
     ]
    }
   ],
   "source": [
    "# Solution 5.b\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('text', data_files={'train': 'alice_in_wonderland.txt'}, split='train')\n",
    "\n",
    "# Function to check token length\n",
    "def check_token_length(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    if len(tokens) < 1:  # Checks if the token length is less than 1\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Filter and extract text data ensuring each line results in at least one token\n",
    "valid_texts = []\n",
    "for item in dataset:\n",
    "    text = item['text'].strip()\n",
    "    if text and check_token_length(text):\n",
    "        valid_texts.append(text)\n",
    "\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(predictions=valid_texts, model_id='gpt2')\n",
    "\n",
    "# map the perplexities to the corresponding text\n",
    "# perplexities = dict(zip(valid_texts, results['perplexities']))\n",
    "# print(perplexities)\n",
    "\n",
    "print(\"Mean Perplexity: \", results['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection Sampling (2 points)\n",
    "\n",
    "Rejection sampling is another approach we can use to generate the next token from the language model. In this approach, we randomly sample a token from the list and accept it if it's probability is greater than some random number and reject it otherwise.\n",
    "\n",
    "1. Generate a random number, r.\n",
    "2. Randomly sample a token from the output probability (say token at index i with probability p).\n",
    "3. Is r < p?\n",
    "\n",
    "    a. If yes, then accept i as the next token.\n",
    "    \n",
    "    b. If no, then go back to step 1.\n",
    "\n",
    "Complete the `rejection_sample()` function in the `GPT2Model` class in `exercise_4.py`. How does the generated output differ from the greedy and random sampling?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution Bonus:\n",
    "\n",
    "Rejection Sampling method introduces a balance between randomness and probability weighted choices. Output is a compromise between random sampling and greedy sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejection sampled output:  Alice was beginning to get very tired of sitting by her sister on the bank   and was starting to think about her own future\n"
     ]
    }
   ],
   "source": [
    "# Random sampling\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.rejection_sample(text)\n",
    "\n",
    "print(\"Rejection sampled output: \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity with different languages (0.5 + 0.5 = 1 point)\n",
    "\n",
    "a. In exercise 5.a., what would happen if we were to use a different language other than English? How would the perplexity be affected? Test this with another language of your choice.\n",
    "\n",
    "b. Are there any pre-trained model for the language you selected? Try with that model to see if you can get a better perplexity score. If there are no pre-trained LM for that language, try with a multilingual LM and see if the perplexity score improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "a. GPT-2 was primarily trained on English text, so predicting the next token in a different language results in a much higher perplexity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbc6968d00e4f2da8e3123e286c0d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Uzbek text:  41356.34375\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load the perplexity metric\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "# Uzbek text to compute perplexity\n",
    "uzbek_text = [\"Evrika! Men topdim!\"]\n",
    "\n",
    "# Compute the perplexity using the GPT-2 model\n",
    "results = perplexity.compute(predictions=uzbek_text, model_id='gpt2')\n",
    "\n",
    "# Print the perplexity result\n",
    "print(\"Perplexity for Uzbek text: \", results['mean_perplexity'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "\n",
    "b. There is a pretrained model for Uzbek language called \"mGPT-1.3B-uzbek\". Using it, the perplexity score decresed from 41356 to 301."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reni/miniconda3/envs/hws_ws23/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca9af40178f46b3823169c3f7560a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Uzbek text:  301.0086364746094\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load the perplexity metric\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "# Uzbek text to compute perplexity\n",
    "uzbek_text = [\"Evrika! Men topdim!\"]\n",
    "\n",
    "# Compute the perplexity using the GPT-2 model\n",
    "results = perplexity.compute(predictions=uzbek_text, model_id='ai-forever/mGPT-1.3B-uzbek')\n",
    "\n",
    "# Print the perplexity result\n",
    "print(\"Perplexity for Uzbek text: \", results['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
