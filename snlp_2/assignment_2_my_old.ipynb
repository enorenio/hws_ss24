{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-21tlRxnfLW"
   },
   "source": [
    "# SNLP Assignment 2\n",
    "\n",
    "Name 1: Aleksey Morshnev<br/>\n",
    "Student id 1: 7042691<br/>\n",
    "Email 1: almo00008@stud.uni-saarland.de<br/>\n",
    "\n",
    "\n",
    "Name 2: <br/>\n",
    "Student id 2: <br/>\n",
    "Email 2: <br/> \n",
    "\n",
    "Name 3: <br/>\n",
    "Student id 3: <br/>\n",
    "Email 3: <br/> \n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook. There is no need to submit the data files. <br/>\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Make sure to click on \"Turn-in\" (or the equivalent on CMS) after you upload your submission, otherwise the assignment will not be considered as submitted. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0o_hqMzc7qAH"
   },
   "source": [
    "## Exercise 1 - Probability theory (0.5 + 0.5 = 1 point)\n",
    "\n",
    "Consider a fair 6-sided die whose sides are numbered from 1 to 6 and each die roll is independent of the other rolls. In an experiment that consists of rolling the die twice, the following events can be defined:\n",
    "\n",
    "    A: The sum of the two outcomes is at least 10\n",
    "    B: At least one of the two rolls resulted in 6\n",
    "\n",
    "a. Compute the probabilities $P(A)$, and $P(B)$.\n",
    "\n",
    "b. Is event A independent of event B?\n",
    "\n",
    "You can leave the solution as a fraction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Bayes theorem (0.5 + 1 = 1.5 points)\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability theory used to update the probability of a hypothesis based on new evidence. It states that the probability of a hypothesis (H) given some evidence (E) is proportional to the probability of the evidence given the hypothesis, multiplied by the prior probability of the hypothesis, divided by the probability of the evidence regardless of the hypothesis. Mathematically,\n",
    "\n",
    "$$ P(H|E) = \\frac{P(E|H)P(H)}{P(E)} $$\n",
    "\n",
    "Where,\n",
    "- $P(H∣E)$ is the posterior probability of hypothesis H given evidence E.\n",
    "- $P(E∣H)$ is the probability of observing evidence E given that the hypothesis H is true.\n",
    "- $P(H)$ is the prior probability of hypothesis H being true.\n",
    "- $P(E)$ is the probability of observing evidence E.\n",
    "\n",
    "\n",
    "Now, Imagine that all of the blogs online were generated by some Large Language Model (LLM). Of these blogs, $80\\%$ were generated using GPT-4, $15\\%$ of them were generated using LLAMA 3 and $5\\%$ were generated using Mistral. About $4\\%$ of the text generated by GPT-4 seem to have counterfactual statements. For LLAMA 3, this is about $6\\%$ and for Mistral, this is about $9\\%$.\n",
    "\n",
    "a. If a blog post is randomly selected from these online blogs, what is the probability that it was generated by LLAMA 3?\n",
    "\n",
    "b. If a randomly selected blog post contained a counterfactual statement, find the probability that it was generated using GPT-4.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Miller's model (0.5 + 1 + 0.5 = 2 points)\n",
    "\n",
    "Miller's model introduced in the lecture simulates the process of random text generation at the character level. It is a memoryless model which means that the generation at each time step is independent of what has been generated so far. Imagine that there is a monkey typing on a computer keyboard. The keyboard has only 27 keys: a to z and a spacebar. Assume that the probability of each character is distributed according to its occurrence in the English language. We call a sequence of characters separated by white space as a word.\n",
    "\n",
    "a. What is the probability that the monkey will type the word _'ear'_ on the keyboard? Hint: See the `exercise_3.py` file.\n",
    "\n",
    "b. Complete the `compute_perplexity()` function in the `exercise_3.py` file to compute the perplexity of the Miller's model for the text: \"hello i am a monkey\". \n",
    "\n",
    "c. What would happen to the perplexity if we were to add a few more whitespace in between the words?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.b\n",
    "\n",
    "# DO NOT MODIFY THIS PART\n",
    "# Your code should be placed in exercise_3.py\n",
    "import exercise_3\n",
    "from importlib import reload\n",
    "\n",
    "reload(exercise_3)\n",
    "\n",
    "model = exercise_3.MillersModel()\n",
    "\n",
    "text = \"hello i am a monkey\"\n",
    "\n",
    "print(\"Perplexity: \", model.compute_perplexity(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Sampling in Language models (1 + 2 + 0.5 + 0.5 =4 points)\n",
    "\n",
    "Neural Networks are quite effective as generative models. Huggingface Transformers library can be used to easily get started with training and using these models. Take a look at the [Huggingface transformers quick tour](https://huggingface.co/docs/transformers/quicktour) to get acquainted with the transformers library. (Recommended: [Chapter 1](https://huggingface.co/learn/nlp-course/chapter1/1) and [Chapter 2](https://huggingface.co/learn/nlp-course/chapter2/1) of the Huggingface NLP Course)\n",
    "\n",
    "a. Read up on GPT2 [[1]](https://huggingface.co/openai-community/gpt2) [[2]](https://huggingface.co/docs/transformers/model_doc/gpt2). Implement the `forward()` function in the `GPT2Model` class in `exercise_4.py` file to take input text sequence and return the probability distribution of the predicted next token. The general steps would be: tokenize and encode the input text, feed the input to the model to get the logits (Note: You might have to take only the last logit which is for the predicted next token in the sequence.), and apply softmax function to convert it to a probability distribution.\n",
    "\n",
    "b. We can use different types of sampling strategies while selecting the next token for generating a text. Some common approaches are: Greedy sampling, random sampling, beam search, etc.\n",
    "\n",
    "   - Greedy sampling\n",
    "      In greedy sampling, the model selects the word with the highest probability based on the given context. It prioritizes the most likely next word based on the model's predictions, without considering other possibilities. This method often produces coherent and fluent text but can lead to repetitive or predictable outputs, as it doesn't explore alternative word choices.\n",
    "\n",
    "   - Random sampling\n",
    "      In random sampling, the next token is randomly sampled from the output probability distribution. This can make the generation non-deterministic and unpredictable but doesn't usually fall into repetitions.\n",
    "\n",
    "   Implement the `greedy_sample()` and `random_sample()` functions in `GPT2Model` class in `exercise_4.py` file for Greedy sampling and random sampling respectively. For each of them, generate a continuation of the sentence given below.\n",
    "\n",
    "c. Describe the output with each sampling method. Which one would you generally prefer?\n",
    "\n",
    "d. Another approach for generating from language models is using Beam search. Describe how the beam search works (2 sentences). Does it solve any problems of the previous sampling methods?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): - WARNING conda.models.version:get_matcher(556): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.7.1.*, but conda is ignoring the .* and treating it as 1.7.1\n",
      "done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 24.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c conda-forge conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.5.0\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/reni/miniconda3/envs/hws_ws23\n",
      "\n",
      "  added / updated specs:\n",
      "    - datasets\n",
      "    - evaluate\n",
      "    - transformers\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    arrow-1.3.0                |     pyhd8ed1ab_0          98 KB  conda-forge\n",
      "    binaryornot-0.4.4          |             py_1         370 KB  conda-forge\n",
      "    cookiecutter-2.6.0         |     pyhca7485f_0          98 KB  conda-forge\n",
      "    datasets-2.19.1            |     pyhd8ed1ab_0         350 KB  conda-forge\n",
      "    evaluate-0.4.1             |     pyhd8ed1ab_0          61 KB  conda-forge\n",
      "    markdown-it-py-3.0.0       |     pyhd8ed1ab_0          63 KB  conda-forge\n",
      "    mdurl-0.1.2                |     pyhd8ed1ab_0          14 KB  conda-forge\n",
      "    pygments-2.18.0            |     pyhd8ed1ab_0         859 KB  conda-forge\n",
      "    python-slugify-8.0.4       |     pyhd8ed1ab_0          15 KB  conda-forge\n",
      "    responses-0.18.0           |     pyhd8ed1ab_0          35 KB  conda-forge\n",
      "    rich-13.7.1                |     pyhd8ed1ab_0         180 KB  conda-forge\n",
      "    text-unidecode-1.3         |     pyhd8ed1ab_1          64 KB  conda-forge\n",
      "    types-python-dateutil-2.9.0.20240316|     pyhd8ed1ab_0          21 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  arrow              conda-forge/noarch::arrow-1.3.0-pyhd8ed1ab_0 \n",
      "  binaryornot        conda-forge/noarch::binaryornot-0.4.4-py_1 \n",
      "  cookiecutter       conda-forge/noarch::cookiecutter-2.6.0-pyhca7485f_0 \n",
      "  evaluate           conda-forge/noarch::evaluate-0.4.1-pyhd8ed1ab_0 \n",
      "  markdown-it-py     conda-forge/noarch::markdown-it-py-3.0.0-pyhd8ed1ab_0 \n",
      "  mdurl              conda-forge/noarch::mdurl-0.1.2-pyhd8ed1ab_0 \n",
      "  pygments           conda-forge/noarch::pygments-2.18.0-pyhd8ed1ab_0 \n",
      "  python-slugify     conda-forge/noarch::python-slugify-8.0.4-pyhd8ed1ab_0 \n",
      "  responses          conda-forge/noarch::responses-0.18.0-pyhd8ed1ab_0 \n",
      "  rich               conda-forge/noarch::rich-13.7.1-pyhd8ed1ab_0 \n",
      "  text-unidecode     conda-forge/noarch::text-unidecode-1.3-pyhd8ed1ab_1 \n",
      "  types-python-date~ conda-forge/noarch::types-python-dateutil-2.9.0.20240316-pyhd8ed1ab_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  datasets                              2.18.0-pyhd8ed1ab_0 --> 2.19.1-pyhd8ed1ab_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "rich-13.7.1          | 180 KB    |                                       |   0% \n",
      "text-unidecode-1.3   | 64 KB     |                                       |   0% \u001b[A\n",
      "\n",
      "evaluate-0.4.1       | 61 KB     |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "binaryornot-0.4.4    | 370 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pygments-2.18.0      | 859 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cookiecutter-2.6.0   | 98 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python-slugify-8.0.4 | 15 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "datasets-2.19.1      | 350 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "types-python-dateuti | 21 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "arrow-1.3.0          | 98 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mdurl-0.1.2          | 14 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "responses-0.18.0     | 35 KB     |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rich-13.7.1          | 180 KB    | ###2                                  |   9% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pygments-2.18.0      | 859 KB    | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "evaluate-0.4.1       | 61 KB     | #########6                            |  26% \u001b[A\u001b[A\n",
      "text-unidecode-1.3   | 64 KB     | #########3                            |  25% \u001b[A\n",
      "\n",
      "\n",
      "binaryornot-0.4.4    | 370 KB    | #6                                    |   4% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pygments-2.18.0      | 859 KB    | ###########                           |  30% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "text-unidecode-1.3   | 64 KB     | ##################################### | 100% \u001b[A\n",
      "\n",
      "evaluate-0.4.1       | 61 KB     | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "evaluate-0.4.1       | 61 KB     | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python-slugify-8.0.4 | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cookiecutter-2.6.0   | 98 KB     | ######                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python-slugify-8.0.4 | 15 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "binaryornot-0.4.4    | 370 KB    | ###################2                  |  52% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "rich-13.7.1          | 180 KB    | ##################################### | 100% \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "types-python-dateuti | 21 KB     | ###########################8          |  75% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pygments-2.18.0      | 859 KB    | ###################9                  |  54% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "types-python-dateuti | 21 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "arrow-1.3.0          | 98 KB     | ######                                |  16% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mdurl-0.1.2          | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "responses-0.18.0     | 35 KB     | ################8                     |  45% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "markdown-it-py-3.0.0 | 63 KB     | #########4                            |  25% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "binaryornot-0.4.4    | 370 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "binaryornot-0.4.4    | 370 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cookiecutter-2.6.0   | 98 KB     | ##############################        |  81% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "arrow-1.3.0          | 98 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "arrow-1.3.0          | 98 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "datasets-2.19.1      | 350 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "datasets-2.19.1      | 350 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "mdurl-0.1.2          | 14 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "responses-0.18.0     | 35 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "responses-0.18.0     | 35 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "markdown-it-py-3.0.0 | 63 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "markdown-it-py-3.0.0 | 63 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pygments-2.18.0      | 859 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "pygments-2.18.0      | 859 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cookiecutter-2.6.0   | 98 KB     | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "# install necessary packages\n",
    "!conda install -y transformers evaluate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 4\n",
    "\n",
    "# DO NOT MODIFY THIS PART\n",
    "# Your code should be placed in exercise_4.py\n",
    "import exercise_4\n",
    "from importlib import reload\n",
    "\n",
    "reload(exercise_4)\n",
    "\n",
    "model = exercise_4.GPT2Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy sampled output:  Alice was beginning to get very tired of sitting by her sister on the bank   and she was starting to get tired of sitting\n"
     ]
    }
   ],
   "source": [
    "# Greedy sampling\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.greedy_sample(text)\n",
    "\n",
    "print(\"Greedy sampled output: \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sampled output:  Alice was beginning to get very tired of sitting by her sister on the bank   while she took herself up on the offer.\n"
     ]
    }
   ],
   "source": [
    "# Random sampling\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.random_sample(text)\n",
    "\n",
    "print(\"Random sampled output: \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Computing Perplexity (0.5 + 1 = 1.5 points)\n",
    "\n",
    "Huggingface provides the `evaluate` library that includes various metrics used in machine learning along with perplexity that we use in Language modeling. We can use that library to quickly compute the perplexity of a language model. There is also the `datasets` library from Huggingface that allows to easily access datasets for many different tasks.\n",
    "\n",
    "a. Read the [metric card for perplexity](https://huggingface.co/spaces/evaluate-metric/perplexity) and compute the perplexity for the following text using GPT2.\n",
    "\n",
    "b. Load the `alice_in_wonderland.txt` file using Hugginface `datasets` library and compute the perplexity for it using GPT2. Check the [documentations](https://huggingface.co/docs/datasets/index) for datasets library.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0189ac717e68427fbcbd342f0ea644ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  24.384519577026367\n"
     ]
    }
   ],
   "source": [
    "# Solution 5.a\n",
    "from evaluate import load\n",
    "\n",
    "texts = [\"Eureka! I have found it!\"]\n",
    "\n",
    "# your code here\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(predictions=texts, model_id='gpt2')\n",
    "\n",
    "print(\"Perplexity: \", results['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40debbc6f9f548cea37e96934afbc067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice was beginning to get very tired of sitting by her sister': 60.3527717590332, 'on the bank, and of having nothing to do:  once or twice she had': 181.5369110107422, 'peeped into the book her sister was reading, but it had no': 97.06128692626953, \"pictures or conversations in it, `and what is the use of a book,'\": 165.4005126953125, \"thought Alice `without pictures or conversation?'\": 9847.4453125, 'So she was considering in her own mind (as well as she could,': 70.94112396240234, 'for the hot day made her feel very sleepy and stupid), whether': 401.9328308105469, 'the pleasure of making a daisy-chain would be worth the trouble': 51.625770568847656, 'of getting up and picking the daisies, when suddenly a White': 152.7297821044922, 'Rabbit with pink eyes ran close by her.': 256.022705078125, 'There was nothing so VERY remarkable in that; nor did Alice': 285.7662658691406, 'think it so VERY much out of the way to hear the Rabbit say to': 326.69989013671875, \"itself, `Oh dear!  Oh dear!  I shall be late!'  (when she thought\": 153.28077697753906, 'it over afterwards, it occurred to her that she ought to have': 82.93213653564453, 'wondered at this, but at the time it all seemed quite natural);': 85.3974838256836, 'but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT-': 660.5038452148438, 'POCKET, and looked at it, and then hurried on, Alice started to': 115.42170715332031, 'her feet, for it flashed across her mind that she had never': 122.67163848876953, 'before seen a rabbit with either a waistcoat-pocket, or a watch to': 367.6038513183594, 'take out of it, and burning with curiosity, she ran across the': 169.58514404296875, 'field after it, and fortunately was just in time to see it pop': 147.66159057617188, 'down a large rabbit-hole under the hedge.': 143.6992950439453}\n",
      "Mean Perplexity:  633.9214832999489\n"
     ]
    }
   ],
   "source": [
    "# Solution 5.b\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('text', data_files={'train': 'alice_in_wonderland.txt'}, split='train')\n",
    "\n",
    "# Function to check token length\n",
    "def check_token_length(text):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    if len(tokens) < 1:  # Checks if the token length is less than 1\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Filter and extract text data ensuring each line results in at least one token\n",
    "valid_texts = []\n",
    "for item in dataset:\n",
    "    text = item['text'].strip()\n",
    "    if text and check_token_length(text):\n",
    "        valid_texts.append(text)\n",
    "\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "results = perplexity.compute(predictions=valid_texts, model_id='gpt2')\n",
    "\n",
    "# map the perplexities to the corresponding text\n",
    "# perplexities = dict(zip(valid_texts, results['perplexities']))\n",
    "# print(perplexities)\n",
    "\n",
    "print(\"Mean Perplexity: \", results['mean_perplexity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection Sampling (2 points)\n",
    "\n",
    "Rejection sampling is another approach we can use to generate the next token from the language model. In this approach, we randomly sample a token from the list and accept it if it's probability is greater than some random number and reject it otherwise.\n",
    "\n",
    "1. Generate a random number, r.\n",
    "2. Randomly sample a token from the output probability (say token at index i with probability p).\n",
    "3. Is r < p?\n",
    "\n",
    "    a. If yes, then accept i as the next token.\n",
    "    \n",
    "    b. If no, then go back to step 1.\n",
    "\n",
    "Complete the `rejection_sample()` function in the `GPT2Model` class in `exercise_4.py`. How does the generated output differ from the greedy and random sampling?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Rejection Sampling method introduces a balance between randomness and probability weighted choices. Output is a compromise between random sampling and greedy sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejection sampled output:  Alice was beginning to get very tired of sitting by her sister on the bank   until her sisters arrived and struggled to free her\n"
     ]
    }
   ],
   "source": [
    "import exercise_4\n",
    "from importlib import reload\n",
    "reload(exercise_4)\n",
    "\n",
    "# Random sampling\n",
    "n_tokens = 10\n",
    "\n",
    "text = \"Alice was beginning to get very tired of sitting by her sister on the bank \"\n",
    "model = exercise_4.GPT2Model()\n",
    "\n",
    "for _ in range(n_tokens):\n",
    "    text += model.random_sample(text)\n",
    "\n",
    "print(\"Rejection sampled output: \", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity with different languages (0.5 + 0.5 = 1 point)\n",
    "\n",
    "a. In exercise 5.a., what would happen if we were to use a different language other than English? How would the perplexity be affected? Test this with another language of your choice.\n",
    "\n",
    "b. Are there any pre-trained model for the language you selected? Try with that model to see if you can get a better perplexity score. If there are no pre-trained LM for that language, try with a multilingual LM and see if the perplexity score improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "a. GPT-2 was primarily trained on English text, so predicting the next token in a different language results in a much higher perplexity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667e6e9dadfd484cae6d834d63730cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Uzbek text:  41356.34375\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load the perplexity metric\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "# Uzbek text to compute perplexity\n",
    "uzbek_text = [\"Evrika! Men topdim!\"]\n",
    "\n",
    "# Compute the perplexity using the GPT-2 model\n",
    "results = perplexity.compute(predictions=uzbek_text, model_id='gpt2')\n",
    "\n",
    "# Print the perplexity result\n",
    "print(\"Perplexity for Uzbek text: \", results['mean_perplexity'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "b. There is a pretrained model for Uzbek language called \"mGPT-1.3B-uzbek\". Using it, the perplexity score decresed from 41356 to 301."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee236cf5f03d4ad78f57b83cb86f6d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/repos/7b/41/7b41f25cbb46d968cf00011078b58c0722841346a3fc3fc65dc24c3a472be3f5/5b742844040495fcbb9db2affe548c5f28b4a22ac27b9a2ea047c4df19fd17e7?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1716319759&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjMxOTc1OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83Yi80MS83YjQxZjI1Y2JiNDZkOTY4Y2YwMDAxMTA3OGI1OGMwNzIyODQxMzQ2YTNmYzNmYzY1ZGMyNGMzYTQ3MmJlM2Y1LzViNzQyODQ0MDQwNDk1ZmNiYjlkYjJhZmZlNTQ4YzVmMjhiNGEyMmFjMjdiOWEyZWEwNDdjNGRmMTlmZDE3ZTc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=RLlvVNgjZD5bnFosmuhvWZyOrE-gI-j8WGCX%7E5D4wu%7EfDAuuQfMbESYSwP0Z8aK%7Eru6-XIAVtTjCXZ4xSNSQc7Y4q6ksVBnyQBSPdpC9HKiCf5ZVbegriqfUcmXBnpO8kFR8NDJDbtxZ1H-uJAsqWHJVsgzsunkwFwdW5bRJ5YrhIS6H9-RBjdAohSWkLmF5bjdUDyK5clz79b7Bkl9kgUq00iOeAOkm%7E%7EsaVVDdhlFxiwsEMW6n6GjoklCeYYLmPuBKggLGYVRTTmtvI9DVOF6FkhoUl5V4Z7Y9TVO8hzcjywK6LjidsSzHzKJoP56QvoWkbisIq05enzuZUO3%7ERw__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e81e6796404e79aaa7ed88bdcc484d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   1%|1         | 62.9M/5.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/repos/7b/41/7b41f25cbb46d968cf00011078b58c0722841346a3fc3fc65dc24c3a472be3f5/5b742844040495fcbb9db2affe548c5f28b4a22ac27b9a2ea047c4df19fd17e7?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1716319759&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjMxOTc1OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83Yi80MS83YjQxZjI1Y2JiNDZkOTY4Y2YwMDAxMTA3OGI1OGMwNzIyODQxMzQ2YTNmYzNmYzY1ZGMyNGMzYTQ3MmJlM2Y1LzViNzQyODQ0MDQwNDk1ZmNiYjlkYjJhZmZlNTQ4YzVmMjhiNGEyMmFjMjdiOWEyZWEwNDdjNGRmMTlmZDE3ZTc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=RLlvVNgjZD5bnFosmuhvWZyOrE-gI-j8WGCX%7E5D4wu%7EfDAuuQfMbESYSwP0Z8aK%7Eru6-XIAVtTjCXZ4xSNSQc7Y4q6ksVBnyQBSPdpC9HKiCf5ZVbegriqfUcmXBnpO8kFR8NDJDbtxZ1H-uJAsqWHJVsgzsunkwFwdW5bRJ5YrhIS6H9-RBjdAohSWkLmF5bjdUDyK5clz79b7Bkl9kgUq00iOeAOkm%7E%7EsaVVDdhlFxiwsEMW6n6GjoklCeYYLmPuBKggLGYVRTTmtvI9DVOF6FkhoUl5V4Z7Y9TVO8hzcjywK6LjidsSzHzKJoP56QvoWkbisIq05enzuZUO3%7ERw__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34114f3d336d4eebb2adf6af39014e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  37%|###7      | 2.16G/5.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/repos/7b/41/7b41f25cbb46d968cf00011078b58c0722841346a3fc3fc65dc24c3a472be3f5/5b742844040495fcbb9db2affe548c5f28b4a22ac27b9a2ea047c4df19fd17e7?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1716319759&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNjMxOTc1OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy83Yi80MS83YjQxZjI1Y2JiNDZkOTY4Y2YwMDAxMTA3OGI1OGMwNzIyODQxMzQ2YTNmYzNmYzY1ZGMyNGMzYTQ3MmJlM2Y1LzViNzQyODQ0MDQwNDk1ZmNiYjlkYjJhZmZlNTQ4YzVmMjhiNGEyMmFjMjdiOWEyZWEwNDdjNGRmMTlmZDE3ZTc%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=RLlvVNgjZD5bnFosmuhvWZyOrE-gI-j8WGCX%7E5D4wu%7EfDAuuQfMbESYSwP0Z8aK%7Eru6-XIAVtTjCXZ4xSNSQc7Y4q6ksVBnyQBSPdpC9HKiCf5ZVbegriqfUcmXBnpO8kFR8NDJDbtxZ1H-uJAsqWHJVsgzsunkwFwdW5bRJ5YrhIS6H9-RBjdAohSWkLmF5bjdUDyK5clz79b7Bkl9kgUq00iOeAOkm%7E%7EsaVVDdhlFxiwsEMW6n6GjoklCeYYLmPuBKggLGYVRTTmtvI9DVOF6FkhoUl5V4Z7Y9TVO8hzcjywK6LjidsSzHzKJoP56QvoWkbisIq05enzuZUO3%7ERw__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac8e69d8c8a47209d48797f2ce41172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  57%|#####7    | 3.30G/5.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reni/miniconda3/envs/hws_ws23/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18aec1451a0144819e865621f3fcfb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713ef236010f4fe1b44dd6f05506da5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41bc0c8dd4940218b77bc125b82cd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc0ebd67d954eddbcfabde8242e3c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85b68563b31479e9c145adadd666403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/582 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34264cf60ff4dd1b92cd6c5b7ac6c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for Uzbek text:  301.0086364746094\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load the perplexity metric\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "\n",
    "# Uzbek text to compute perplexity\n",
    "uzbek_text = [\"Evrika! Men topdim!\"]\n",
    "\n",
    "# Compute the perplexity using the GPT-2 model\n",
    "results = perplexity.compute(predictions=uzbek_text, model_id='ai-forever/mGPT-1.3B-uzbek')\n",
    "\n",
    "# Print the perplexity result\n",
    "print(\"Perplexity for Uzbek text: \", results['mean_perplexity'])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
