{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv7J5tBx6KcM"
   },
   "source": [
    "# SNLP Assignment 3\n",
    "\n",
    "Name 1: Daniya<br>\n",
    "Student id 1: 7062343 <br>\n",
    "Email 1: <br>\n",
    "\n",
    "Name 2: <br>\n",
    "Student id 2:  <br>\n",
    "Email 2:  <br>\n",
    "\n",
    "Name 3: Aleksey Morshnev<br/>\n",
    "Student id 3: 7042691<br/>\n",
    "Email 3: almo00008@stud.uni-saarland.de<br/>\n",
    "\n",
    "**Instructions:** Read each question carefully. <br/>\n",
    "Make sure you appropriately comment your code wherever required. Your final submission should contain the completed Notebook and the respective Python files for any additional exercises necessary. There is no need to submit the data files should they exist. <br>\n",
    "**In this case, however, submit your randomized text for `randomization_parameter` = 0.2 from exercise 2.2, so that we can verify your code.**  <br/>\n",
    "\n",
    "Upload the zipped folder on CMS. Please follow the naming convention of **Name1_studentID1_Name2_studentID2_Name3_studentID3.zip**. Only one member of the group should make the submisssion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General guidelines:\n",
    "1. Don't change the layout of the notebook, especially the final output cells. If you do end up changing the code, add comments to tell us why you needed to do it.\n",
    "2. Use log with base 2 wherever you feel logarithimic operations are necessary. If you do use other bases, please specify it and explain why.\n",
    "3. For tokenizers, it is sufficient to use tokenizers from `nltk` library.\n",
    "4. Always use log with base 2 for your logarithmic operations. \n",
    "5. For the theoritical questions where no math is involved, it is sufficient to answer the questions with 3-4 sentences. Do not be too verbose.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OazjYvC9C6Mi"
   },
   "source": [
    "# Exercise 1 - Joint and conditional entropy (1 point)\n",
    "\n",
    "In this exercise you will review and work on information theory concepts.\n",
    "\n",
    "We define the joint entropy as:\n",
    "\n",
    "$$ H(X,Y) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y) \\log p(x,y) $$\n",
    "\n",
    "We define the conditional entropy as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H(Y|X) & = \\sum_{x \\in \\mathcal{X}} p(x,y) H(Y|X=x) \\\\\n",
    "& = \\sum_{x \\in \\mathcal{X}} p(x) \\left[ - \\sum_{y \\in \\mathcal{Y}}p(y|x) \\log p(y|x) \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Show that $H(X,Y) = H(X) + H(Y|X)$ or that $ H(Y|X) = H(X,Y) - H(X) $ holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "We Know that \n",
    "$$ H(X,Y) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y) \\log p(x,y)  $$ \n",
    "\n",
    "$$ p(x,y) = p(y|x) . p(x) $$ \n",
    "\n",
    "H(X,Y) can be writtern as \n",
    "\n",
    "$$ H(X,Y) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y) \\log p(y|x) . p(x)  $$ \n",
    "since\n",
    "$$  logAB = logA + logB $$   then \n",
    "\n",
    "$$ H(X,Y) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y) \\left[ \\log p(y|x) +  logp(x) \\right]  $$ \n",
    "\n",
    "$$ H(X,Y) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y) \\log p(y|x) - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y)logp(x)  $$ \n",
    "\n",
    "If we notice the first term \n",
    "\n",
    "$$ H(Y|X) =- \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y) \\log p(y|x) $$\n",
    "\n",
    "So H(X,Y) can be writtern as \n",
    "$$ H(X,Y) = H(Y|X) - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x,y)logp(x)  $$ \n",
    "\n",
    "we know that \n",
    "\n",
    "$$  \\sum_{y \\in \\mathcal{Y}} p(x,y) = p(x) $$\n",
    "\n",
    "\n",
    "$$ H(X,Y) = H(Y|X) - \\sum_{x \\in \\mathcal{X}} p(x) logp(x)  $$ \n",
    "\n",
    "now simple by the definition of entropy \n",
    "$$ H(X) = - \\sum_{x \\in \\mathcal{X}} p(x) logp(x)  $$ \n",
    "\n",
    "Hence \n",
    "\n",
    "$$ H(X,Y) = H(Y|X) + H(X)  $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-KQRz1cHrPi"
   },
   "source": [
    "# Exercise 2: Perplexity and Entropy\n",
    "\n",
    "In this exercise, you will explore the concept of entropy and perplexity in the context of text analysis. You will calculate these measures for a given text corpus, investigate how they change when the text is altered, and analyze the impact of combining texts from different languages.\n",
    "\n",
    "### Exercise 2.1: Perplexity and Entropy calculation (2 points)\n",
    "\n",
    "You're given the text of Alice in Wonderland: `alice_eng.txt`. Now using the text file, perform the following steps.\n",
    "\n",
    "1. Load the text and tokenize and lowercase each token. Filter out the puncatuation also. (0.5 points)\n",
    "\n",
    "2. Compute $P(i,j)$, which is the probability that at any position in the text you will find the word $i$ followed immediately by the word j, and $P(j|i)$, which is the probability that if word $i$ occurs in the text then word $j$ will follow. (0.5 points)\n",
    "\n",
    "3. Calculate the the conditional entropy of the word distribution which can also be written as: (0.5 point) $$H(J|I) = -\\sum_{i \\in I, j \\in J} P(i,j)log_{2}P(j|i)$$\n",
    "\n",
    "4. And then, the perplexity: (0.5 points)$$PP = 2^{H(J|I)}$$\n",
    "\n",
    "To accomplish all the above, complete the functions `compute_probabilities()`, `compute_entropy()` and `compute_perplexity()`, then run the driver code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1715812018410,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -120
    },
    "id": "W1y7_V-W5-ey",
    "outputId": "dbbca92b-a85b-4097-9640-9fc85f6a3bdd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/reni/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from math import log2\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1715812018657,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -120
    },
    "id": "C3tbQFsuC37Y"
   },
   "outputs": [],
   "source": [
    "def compute_probabilities(filename):\n",
    "    \"\"\"\n",
    "    Compute the probabilities P(i, j) and P(j | i) for a given text.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The name of the text file.\n",
    "\n",
    "    Returns:\n",
    "    p_ij (dict): A dictionary where the keys are bigrams (i, j) and the values are the probabilities P(i, j).\n",
    "    p_j_given_i (dict): A dictionary where the keys are bigrams (i, j) and the values are the conditional probabilities P(j | i).\n",
    "    \"\"\"\n",
    "\n",
    "    with open(filename, 'r',encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # creating bigrams - word pair that can be found at any given point in text\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "    bigram_freq = Counter(bigrams)\n",
    "    \n",
    "#     print(bigram_freq)\n",
    "\n",
    "    count_tokens_unigram = Counter(tokens)\n",
    "    \n",
    "    total_bigrams_freq = sum(bigram_freq.values())\n",
    "    \n",
    "    # how many time i and j occur consequectively out of totals bigrams\n",
    "    p_ij = {bigram: bigram_freq / total_bigrams_freq for bigram, bigram_freq in bigram_freq.items()}\n",
    "    \n",
    "    # how many time i occur followed by j( i,j) bigram / how many time i occur \n",
    "    p_j_given_i = {bigram: bigram_freq / count_tokens_unigram[bigram[0]] for bigram, bigram_freq in bigram_freq.items()}\n",
    "    \n",
    "        \n",
    "    return p_ij, p_j_given_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715812019770,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -120
    },
    "id": "k77G3eHEwybI"
   },
   "outputs": [],
   "source": [
    "def compute_entropy(p_ij, p_j_given_i):\n",
    "    \"\"\"\n",
    "    Compute the conditional entropy of the word distribution in a text given the previous word.\n",
    "\n",
    "    Parameters:\n",
    "    p_ij (dict): A dictionary where the keys are bigrams (i, j) and the values are the probabilities P(i, j).\n",
    "    p_j_given_i (dict): A dictionary where the keys are bigrams (i, j) and the values are the conditional probabilities P(j | i).\n",
    "\n",
    "    Returns:\n",
    "    float: The conditional entropy of the word distribution in the text given the previous word.\n",
    "    \"\"\"\n",
    "    return -sum(pij * log2(p_j_given_i[bigram]) for bigram, pij in p_ij.items())\n",
    "\n",
    "\n",
    "def compute_perplexity(entropy):\n",
    "    \"\"\"\n",
    "    Compute the perplexity of a text given its entropy.\n",
    "\n",
    "    Parameters:\n",
    "    entropy (float): The entropy of the text.\n",
    "\n",
    "    Returns:\n",
    "    float: The perplexity of the text.\n",
    "    \"\"\"\n",
    "    return 2 ** entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1715812020496,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -120
    },
    "id": "BJ9MT6SPwvHp",
    "outputId": "bdd25759-c000-41bf-aee4-e4d977ef3667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 3.7743973412817073, Perplexity: 13.68380306549025\n"
     ]
    }
   ],
   "source": [
    "p_ij, p_j_given_i = compute_probabilities('data/alice_eng.txt')\n",
    "entropy = compute_entropy(p_ij, p_j_given_i)\n",
    "perplexity = compute_perplexity(entropy)\n",
    "\n",
    "print(f'Entropy: {entropy}, Perplexity: {perplexity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGIyKs9Mxh91"
   },
   "source": [
    "### Exercise 2.2: Altering the text (2 points)\n",
    "\n",
    "Now, take the text, randomize its contents and perform the following operations:\n",
    "\n",
    "1. The `randomization` parameter determines the fraction of words that will be replaced. If parameter is $n$, randomly select $n$% of words from the text and replace it with a randomly chosen word from the set of words that appear in the text. (0.5 points)\n",
    "\n",
    "2. Since there is some randomness to the outcome of the experiment, run the experiment 5 times. From all 5 trials, calculate the minimum, maximum, and average entropy from the experiments. Also calculate average perplexity.(0.5 points)\n",
    "\n",
    "\n",
    "Do this experiment with different `randomization` values: 1%, 5%, 10%, 20%.\n",
    "\n",
    "To accomplish all the above, complete the functions `randomize_text()`, `run_experiment()`, then run the driver code below\n",
    "\n",
    "**Note**: \n",
    "1. Save the randomized version of the text for the `randomization` parameter: 0.2, you will need it for Exercise 3.\n",
    "2. Use the supplied seed parameter `seed_val`, don't change the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1715812024793,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -120
    },
    "id": "7e8qksa4zaKC"
   },
   "outputs": [],
   "source": [
    "def randomize_text(filename, randomization):\n",
    "    \"\"\"\n",
    "    Randomize a fraction of words in a text.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The name of the text file.\n",
    "    randomization (float): The fraction of total words that will be replaced with a random word.\n",
    "\n",
    "    Returns:\n",
    "    str: The randomized text.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    word_count = len(tokens)\n",
    "    word_count_to_replace = int(word_count * randomization)\n",
    "    \n",
    "    random_indices_to_replace = random.sample(range(word_count), word_count_to_replace)\n",
    "    words_to_replace = [tokens[i] for i in random_indices_to_replace]\n",
    "    \n",
    "    word_bucket = list(set(tokens))\n",
    "    words_for_replacement = random.choices(word_bucket, k=word_count_to_replace)\n",
    "    \n",
    "    randomized_tokens = tokens[:]\n",
    "    for i, word_index in enumerate(random_indices_to_replace):\n",
    "        randomized_tokens[word_index] = words_for_replacement[i]\n",
    "    \n",
    "    return ' '.join(randomized_tokens)\n",
    "    \n",
    "\n",
    "    pass\n",
    "\n",
    "def run_experiment(filename, randomization, seed_val=0):\n",
    "    \"\"\"\n",
    "    Run an experiment to randomize a text and compute the entropy of the resulting text.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The name of the text file.\n",
    "    randomization (float): The fraction of total words that will be replaced with a random word.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the minimum, maximum, and average entropy of the randomized text over 5 runs of the experiment.\n",
    "    \"\"\"\n",
    "    randomized_text_02 = ''\n",
    "    random.seed(seed_val)\n",
    "    trail5_entropies = []\n",
    "    with open('data/randomized_text.txt', 'w', encoding='utf-8') as file:\n",
    "        for _ in range(5):\n",
    "            randomized_text = randomize_text(filename, randomization)\n",
    "            if randomization ==0.2:\n",
    "                randomized_text_02 = randomized_text\n",
    "            file.write(randomized_text)\n",
    "            p_ij, p_j_given_i = compute_probabilities('data/randomized_text.txt')\n",
    "            entropy = compute_entropy(p_ij, p_j_given_i)\n",
    "            trail5_entropies.append(entropy)\n",
    "                 \n",
    "         \n",
    "    min_entropy = min(trail5_entropies)\n",
    "    max_entropy = max(trail5_entropies)\n",
    "    avg_entropy = sum(trail5_entropies) / len(trail5_entropies)\n",
    "    return min_entropy, max_entropy, avg_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2745,
     "status": "ok",
     "timestamp": 1715812028636,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -120
    },
    "id": "_W-717l8zdMj",
    "outputId": "30e86146-f2c1-4fa0-c5e8-fafba033e301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Entropy: 3.77728634670821, Max Entropy: 3.8321017436121187, Average Entropy: 3.811535820906638, Average perplexity: 14.040630540267232 at randomization parameter = 1.0%\n",
      "Min Entropy: 3.790212189425569, Max Entropy: 4.052989722287807, Average Entropy: 3.953369259125084, Average perplexity: 15.491116888566669 at randomization parameter = 5.0%\n",
      "Min Entropy: 3.7859714490353915, Max Entropy: 4.288856359089383, Average Entropy: 4.099221013591983, Average perplexity: 17.13911859634187 at randomization parameter = 10.0%\n",
      "Min Entropy: 3.7478386503631915, Max Entropy: 4.69145257541061, Average Entropy: 4.325341567548466, Average perplexity: 20.047376804599946 at randomization parameter = 20.0%\n"
     ]
    }
   ],
   "source": [
    "# Run the experiment for 5 iterations\n",
    "randomization_params = [0.01, 0.05, 0.1, 0.2]\n",
    "for randomization in randomization_params:\n",
    "  min_entropy, max_entropy, avg_entropy = run_experiment('data/alice_eng.txt', randomization)\n",
    "  print(f'Min Entropy: {min_entropy}, Max Entropy: {max_entropy}, Average Entropy: {avg_entropy}, Average perplexity: {compute_perplexity(avg_entropy)} at randomization parameter = {randomization*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrId2yZWBXTf"
   },
   "source": [
    "After running the above experiments, answer the below questions: (1 point)\n",
    "\n",
    "1. How does altering a text (like randomizing or messing up words and characters) affect its entropy and perplexity in your opinion? Does the results from the experiment match with your expectations? \n",
    "\n",
    "  A: Yes for 10% changes to 20% changes the amount of surprise is increased by fair margin. Perpexiity has a visible increase as well.\n",
    "\n",
    "2. How would the entropy and perplexity change if we were to consider trigrams (sequences of three words) instead of bigrams?\n",
    "\n",
    "  A: Below are the results with trigram and trigrams shows the similar consectiv changes except that our entropy for more no sequence of words (trigram) is increased as compared to bigrams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Min Entropy: 4.689633386584966, Max Entropy: 4.789657476201698, Average Entropy: 4.753372750971574, Average perplexity: 26.971666348014736 at randomization parameter = 1.0%\n",
    "Min Entropy: 4.6050728566323675, Max Entropy: 5.069730863722908, Average Entropy: 4.897937993207064, Average perplexity: 29.81441229173263 at randomization parameter = 5.0%\n",
    "Min Entropy: 4.504937627650465, Max Entropy: 5.349393203555398, Average Entropy: 5.035784333628605, Average perplexity: 32.80364747660956 at randomization parameter = 10.0%\n",
    "Min Entropy: 4.288279069016113, Max Entropy: 5.718496594586029, Average Entropy: 5.163317885047535, Average perplexity: 35.83550760312901 at randomization parameter = 20.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiBVm6rrRN8N"
   },
   "source": [
    "### Exercise 2.3 : Character level analysis (2 points)\n",
    "\n",
    "Perform the same operation as the previous exercise, but this time with characters instead of words, i.e. consider the `randomization_parameter` to be the fraction of characters, not words to be replaced randomly. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1715812119579,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -120
    },
    "id": "Nwcf0z41UX1r"
   },
   "outputs": [],
   "source": [
    "def randomize_char(filename, likelihood):\n",
    "    \"\"\"\n",
    "    Randomize a fraction of characters in a text.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The name of the text file.\n",
    "    likelihood (float): The fraction of total characters that will be replaced with a random character.\n",
    "\n",
    "    Returns:\n",
    "    str: The randomized text.\n",
    "    \"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "def run_experiment_char(filename, randomization, seed_val=42):\n",
    "    \"\"\"\n",
    "    Run an experiment to randomize a text and compute the entropy of the resulting text.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The name of the text file.\n",
    "    likelihood (float): The fraction of total characters that will be replaced with a random character.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the minimum, maximum, and average entropy of the randomized text over 5 runs of the experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2075,
     "status": "ok",
     "timestamp": 1715812193185,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -120
    },
    "id": "qPge3fnmUait",
    "outputId": "708454d5-90ad-47e8-b329-8d67e68a54a9"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m randomization_params \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m randomization \u001b[38;5;129;01min\u001b[39;00m randomization_params:\n\u001b[0;32m----> 4\u001b[0m   min_entropy, max_entropy, avg_entropy \u001b[38;5;241m=\u001b[39m run_experiment_char(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/alice_eng.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, randomization)\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMin Entropy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_entropy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Max Entropy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_entropy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Entropy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_entropy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompute_perplexity(avg_entropy)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at randomization parameter = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrandomization\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Run the experiment for 5 iterations\n",
    "randomization_params = [0.01, 0.05, 0.1, 0.2]\n",
    "for randomization in randomization_params:\n",
    "  min_entropy, max_entropy, avg_entropy = run_experiment_char('data/alice_eng.txt', randomization)\n",
    "  print(f'Min Entropy: {min_entropy}, Max Entropy: {max_entropy}, Average Entropy: {avg_entropy}, Average perplexity: {compute_perplexity(avg_entropy)} at randomization parameter = {randomization*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRYHFY76RooM"
   },
   "source": [
    "After running the above experiments, answer the below questions: (1 point)\n",
    "\n",
    "1. What changes do you observe in the entropy values? Why do you think the values are different from the ones at the word-level? (0.5 points)\n",
    "\n",
    "  A: \n",
    "\n",
    "2. If the same operation was performed on a character-level in another language, (for instance German), do you think the entropy will be more or less? (0.5 points)\n",
    "\n",
    "  A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLWPpuXmUXzw"
   },
   "source": [
    "# Exercise 3:  KL-Divergence (3 points)\n",
    "\n",
    "Kl-Divergence is a measure of how dissimilar two different probability distributions are from each other. It is used in language modelling by comapring different distributions with each other. It is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(P\\|Q) = \\sum_{x \\in X}P(x) \\cdot \\log \\frac{P(x)}{Q(x)}\n",
    "\\end{equation}\n",
    "\n",
    "Where $P$ is the empirical or observed distribution, and Q is the estimated distribution over a common probabilitiy space $X$.\n",
    "\n",
    "In this exercise, you will be comparing the probability distrubutions of Alice in Wonderland: `alice_eng.txt` with it's German edition (`alice_ger.txt`) as well as the 20% randomized version of the English edition. To do that, follow the steps:\n",
    "\n",
    "1. Read the text files and tokenize the text into individual words and filter out punctuation. (0.5 points)\n",
    "\n",
    "2. Compute the frequency distribution of the unigrams from their respective texts. (0.5 points)\n",
    "\n",
    "3. Compute the probabilities of each word (unigram) in their respective texts using the frequency distribution. (0.5 points)\n",
    "\n",
    "4. Use these probabilities to calculate the KL Divegence. If a word from the first distribution is not available in the 2nd distribution, then do not include it in the divergence calculation. (0.5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1715812125188,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -120
    },
    "id": "ULjgDH9LQcAJ"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    words = [token for token in tokens if token.isalpha()]\n",
    "    return words\n",
    "\n",
    "def calculate_kl_divergence(file1, file2):\n",
    "    \"\"\"\n",
    "    Calculate the Kullback-Leibler (KL) Divergence between two text files.\n",
    "\n",
    "    This function reads two text files, tokenizes the text into individual words, calculates the frequency distribution of words in each text, computes the probabilities of each word in each text, and finally calculates the KL divergence.\n",
    "\n",
    "    Parameters:\n",
    "    file1 (str): The name of the first text file.\n",
    "    file2 (str): The name of the second text file.\n",
    "\n",
    "    Returns:\n",
    "    kl_divergence (float): The KL divergence between the two texts. For each word in the first text, if the word also exists in the second text, the product of the probability of the word in the first text and the logarithm base 2 of the ratio of the probability of the word in the first text to the probability of the word in the second text is added to the KL divergence.\n",
    "    \"\"\"\n",
    "    # Read the files\n",
    "    with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2:\n",
    "        text1 = f1.read()\n",
    "        text2 = f2.read()\n",
    "    # Remove punctuation\n",
    "    # Tokenize the text\n",
    "    words1 = tokenize(text1)\n",
    "    words2 = tokenize(text2)\n",
    "    # Get the frequency distribution\n",
    "    freq1 = Counter(words1)\n",
    "    freq2 = Counter(words2)\n",
    "    # Calculate the probabilities\n",
    "    total1 = sum(freq1.values())\n",
    "    total2 = sum(freq2.values())\n",
    "    prob1 = {word: count / total1 for word, count in freq1.items()}\n",
    "    prob2 = {word: count / total2 for word, count in freq2.items()}\n",
    "    # Calculate the KL divergence\n",
    "    kl_divergence = 0.0\n",
    "    for word, p in prob1.items():\n",
    "        if word in prob2:\n",
    "            q = prob2[word]\n",
    "            kl_divergence += p * math.log2(p / q)\n",
    "    \n",
    "    return kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1143,
     "status": "ok",
     "timestamp": 1715812149128,
     "user": {
      "displayName": "Monseej Purkayastha",
      "userId": "09437515445831882048"
     },
     "user_tz": -120
    },
    "id": "vz6L9alapLa1",
    "outputId": "aee98bcf-a20f-453a-e35d-b96666f1bf8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The KL-divergence between the text and its translation is: 1.7045932170593383\n",
      "The KL-divergence between the text and its 20% randomized version is: 0.07172961866716618\n"
     ]
    }
   ],
   "source": [
    "kl_divergence_eng_de = calculate_kl_divergence('data/alice_eng.txt', 'data/alice_ger.txt')\n",
    "kl_divergence_eng_random = calculate_kl_divergence('data/alice_eng.txt', 'data/randomized_text.txt')\n",
    "print(f'The KL-divergence between the text and its translation is: {kl_divergence_eng_de}')\n",
    "print(f'The KL-divergence between the text and its 20% randomized version is: {kl_divergence_eng_random}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjdMOF4WgY-y"
   },
   "source": [
    "After running the above experiments, answer the below questions: (1 point)\n",
    "\n",
    "1. What do you think of the divergence values you have calculated? Do they match with your expectations?\n",
    "\n",
    "  A: Divergence values match my expectations since comparing text to its translation is expected to result in a higher divergence value. Different language uses different vocabulary and structures.\n",
    "\n",
    "2. Do you think this is a good way to compare different versions of a text?\n",
    "\n",
    "  A: Yes, using KL divergence is a good way to compare different versions of a text because it quantitatively measures how one text's word probability distribution is different from another's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKukJ_T9YLLY"
   },
   "source": [
    "# Exercise 4 (Bonus): Language Models and Compression (3 points)\n",
    "\n",
    "This is a reading asignment, read the following two papers and answer the below questions.\n",
    "\n",
    "Paper 1: [Language Modelling is Compression](https://arxiv.org/pdf/2309.10668)<br>\n",
    "Paper 2: [Low-Resource Text Classification](https://aclanthology.org/2023.findings-acl.426.pdf)\n",
    "\n",
    "Q1. Paper 1 draws relationships between probabilistic ML models and lossless compression techniques. Why not the same parallels with lossy compression methods?\n",
    "\n",
    "  A: The paper discusses the usage of ML model on a task of a lossless compression. It uses methods to evaluate performance of lossless compression. For the task of lossy compression, there is a need of different methods. Of course it is possible, but the paper focuses of lossless one.\n",
    "\n",
    "Q2. Looking over the results of Paper 1, do you think there is any advantage is using LLMs as compressors? What could be a possible roadblock?\n",
    "\n",
    "  A: Certainly there's an advantage to using LLM as compressors, but it should be aknowledged that transformers require a lot of compute and correspondingly many predictive tasks require long contexts. Extending these models' context length is a big roadblock. There is, however, an advantage - it can be used not by a casual user, but provided as a service. Putting the model into a powerful machine and providing results in a cloud seems like a good idea.\n",
    "\n",
    "Q3. In Paper 2, why do you think the gzip-based algorithm performs so well on OOD datasets when compared to more traditional approaches?\n",
    "\n",
    " A: Gzip is inherently good at capturing redundancies in data. In OOD datasets, this ability of gzip helps identify underlying patterns that are not specific but common across different datasets (it's data-type-agnostic and it helps to handle unseen datasets). Also, gzip does not introduce inductive bias during training, unlike the BERT and mBERT models, hence it can generalize better to OOD datasets. Gzip method does not involve training parameters (it's non-parametric), so it avoids overfitting naturally, unlike DNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOiPGZRYXBEzi2LiryTjoxd",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
